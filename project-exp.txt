hello, I'm rajani having 9+ years as sre/devops role,
I'm currently working in CME, leading derivatives market place.
I have worked in multiple domains like banking , insurance and Airline

TECHNICAL SKILLS:

VPC and VPC CIDR decided by arc
Public which subnet have internet access, IGW
private subnets which subnet doesnt have internet access — NAT gatewat (whic will available in public subnet)
db servers which should not directly expose to public,
internet gateway which assigned to VPC
security groups, NAcls, routes

using terraform you have created the necessary infrastructure such as a VPC, SGs, ALB, RDS, ACM.
->We have created the eks using terraform and cluster upgrades also we did it using terraform.
->Using Jenkins and its shared libraries we have ensured that your application code is bug-free and secure too.
by using quality gates (code quality )
sonar cube - code quality/vulnerabilities
trivyscan - containers

->Later on you build the image and push the image to a private ECR using Docker.

->At last i have deployed the application to the EKS cluster.
->Hope this gives you a better understanding of how to integrate all the tools together.

->Then 
 we have to update aws kube config in workstation which use for application deployment.  
 (aws eks update-kubeconfig --region us-east-1 expense-dev)
 (post running this command will get .kube folder and which we have authencation and authorisation)


github-jenkins(container/ECR/helm chats   sonarcube-code vulnarabilities)

comming to daily activities our project is like financial sector its hihgly secure. so we are following mostly blue green deployment

1. rolling updates  : rolling update have no zero downtime
ex: you have 4 replicas for 1 application and when you deploying new version first out 4, 1 will change to new 
_____________________

2. this we can keep when we create a canary service canary: 5% of production users in ingress service 
https://kubernetes.github.io/ingress-nginx/examples/canary/  (fyi, you will get all info)
ingress contoller installation-ingresss service

_________________________
blue & green is safe and costly:
will create new deployment in green nodes, 
we redirect the the traffic from ingreess by changing the service to green, load balancer will point to green

-----------------------

cluster upgrade:
1. blue and green nodes should be available
2. traffic will be available only in any of the nodes either in blue or green
3. the new nodes for example green are new then we should cordon(not ready for scheduling) the nodes
4.new nodes should cordon as this green nodes will not have any pods
(scenario:if any issue with control plane , we dont see any issue for application. 
Bcz nodes are associated with alb, 
new deployments will fail if control plane is down and existing app will not cause any issue if control plane is down)
5. once cluster updated, green nodes also update it.
6. uncordon green nodes
7. drain the blues nodes

1.st will deploy in green nodes
2. will redirect the traffic using loadbalancer
blue -
green - 

webhook 

ps -ef 
logs
imagepull back off 
crashloop 
selectors

load balancers
classic load balancer ( which is deprecated now)
application load balancer (path based/context based/host based — layer 7 http/htpps)
network load balancer (milliones of reqs per second — web servers — tcp/tls, udp traffic)
Alb as target to NLB

cross accounts,
ec2 -s3
interface endpoints / gateway endpoints
instead of allowing the traffic to reach the ec2 instances over the internet,

2 vpcs connection — vpc peering
2 different cidr blocs
we have to accept the request
we have to add the route

10 vpcs — transist gateways

AWS lambda — serverless architectre ,

RDS — credentilas
ssm paramets store, AWS secret manager and we can use them
IAM role — policy

load balancer — health check — ttl
load balancer will not send traffic to unhealthy servers

Autoscalling — based on demand
lauch template
tragets
lister
lister rule

application is fine — 1 server is down

high availability — 1 node is down 1 instance will up

servers getting unhealthy — after how to check — latest deployments cause

SNS — email id / slack id , it will send mails we have to subcribe them

authentication — IAM

r53 — dns records
A record IPV4 — alias records
c name — canonical ,
load balancer dns — www. abc.com — A record -

ec2 — no IGW
gateway endpoints

terraform backend
bucket name
key
dynamo db table name

.tf extension
terraform init — install the backend and install the dependicies and plug ins
terraform validate
terraform plan
terraform apply
terraform destroy

idempotence:
n number of times, it will execute
state file
dynamo db — lock hcl file

provider.tf
variable.tf
main.tf
aws configure

cicd
aws secret manager — user id and password
using datasource — will get user, password

jenkins agent
install aws cli agent, IAM role to agent/ aws configure

a/c A, B — trust
alias — provider

identifier — db
terraform traget
terraform import

scripting
1. created shell delete the old logs
shebang
sourcedir
need to validate the dir exist or not
$? previous command
find “$LOG_DIR” -type f -mtime +$RETENTION_DAYS -exec rm -f {} \;

dev
prepod
uat
ci/cd
main — prod
feature -dev
jenkins — gcp
master nodes
worked nodes — manager jenkins-laugh via ssh
aws to gcp

jenkins agent have aws cli, aws configure, docker eks,terraform, kubectl client,
aws eks update-kubeconfig — region us-east-1 — name expense-dev — -.kube file for authentication /authorisation

k8s
hpa

docker images
vm — its very huge memory 1 gb — unneccesry packages — separate guest os
docker — less momory — lightweight (no ping, nestart)

kubernates — isolation
docket network
default
docket 0
virtual eth

brgidge network : outside
container port
host port

multistage build
build jdk

running container jre

users-ingress controller — ingress service — services — pod

user r53- ingress -namespaces — services — labels/selectros

services: cluser ip/ node port / loadbalancer

deployment — stateless application

sateful set — headless servicess — sequential
aws csi drivers — iam role — service account

service account -oidc — aws services

pod to pod — services — service names

ingress services — pod

IQ: Networking in K8s?
— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
Kubernetes Networking:
1. Pod-to-Pod Communication: Each Pod->own IP->allowing direct communication->other Pods across nodes-> cluster.
2. Services: Kubernetes provides Services with virtual IPs (ClusterIPs) to allow access to Pods in a stable and scalable way.
3. DNS: Kubernetes automatically assigns DNS names to Services, allowing Pods to communicate using names instead of IP addresses.
4. Ingress: For HTTP(S) traffic, Ingress resources can route external requests to internal Services based on path or hostname.
5. Network Policies: Kubernetes allows you to define Network Policies to control communication between Pods for security.
6. CNI Plugins: Kubernetes uses CNI plugins to manage networking, ensuring Pods get IP addresses and can communicate across the cluster.


nestat -lntp
lsof 
ss - tulpn

merging code - PR
git checkout main

code quality - quality gates 
sonar cube - code quality/vulnerabilities
trivyscan - 

Lamda - serverless / Cost optimisation  - need to check
ec2 creating/destroy
ebs 

cost optimisition of lamda in ebs

server is able to access internet - public - inergateway - sg /80/443 open & ssh
appln - private subnet - ec2 - NAT Gateway 
if server is private - sg /80/443 open & ssh
NACL rules of inbound 
incoming traffic - 
outbond - download

db appln - private subent 
front -db 


access denied - chmod/chown - grant user    
cnmod -r 4,w3,x1
777
IAM role 




